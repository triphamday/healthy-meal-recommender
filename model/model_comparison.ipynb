{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["wvj6PrpP1kjm","OEZT3Qt1ALfF","uM0ByJBY_GEw","RZy1A7eI1fCO"],"mount_file_id":"105q1jZNaAKq7t6nq1_Y3_Rs5Sd1S-sGW","authorship_tag":"ABX9TyONrRIuwv80Lm68A8LAGfis"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Metrics"],"metadata":{"id":"4k2xochh39nS"}},{"cell_type":"code","source":["def calculate_ndcg(recommendations, relevance_scores, k=10):\n","    recommendations = recommendations[:k]\n","    relevance_scores = relevance_scores[:k]\n","\n","    dcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores)])\n","    idcg = sum([1.0 / np.log2(idx + 2) for idx in range(len(relevance_scores))])\n","\n","    ndcg = dcg / idcg if idcg > 0 else 0.0\n","    return ndcg\n","\n","def calculate_coverage(recommendations, total_items):\n","    unique_recommendations = set(recommendations['Recipes ID'])\n","    coverage = len(unique_recommendations) / total_items\n","    return coverage"],"metadata":{"id":"qd-rJva3544n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#KNN"],"metadata":{"id":"F5y6r4X81m2u"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufV7UChvw3dE","executionInfo":{"status":"ok","timestamp":1719243563807,"user_tz":-420,"elapsed":1536,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"65eb9b35-d0f1-4cb6-d32b-48f4b2fe5c61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Filtered Data Shape: (2344, 15)\n","Extracted Data Shape: (2344, 15)\n","Number of neighbors used: 5\n","Indices of Neighbors: [1008  316  107  268  267]\n","Number of Recommendations: 5\n","Similarities: [[0.974178   0.95229378 0.95181954 0.94262861 0.93635933]]\n","Recommended Recipes:\n","     Recipes ID  Time (mins)  Calories  Total Fat (g)  Saturated Fat (g)  \\\n","698         699         40.0      29.7            1.9                0.7   \n","\n","     Cholesterol (mg)  Sodium (mg)  Total Carbohydrate (g)  Dietary Fiber (g)  \\\n","698               8.7         22.9                     0.5                0.1   \n","\n","     Sugars (g)  Protein (g) Diet Label  \\\n","698         0.1          2.4  unlabeled   \n","\n","                                            Ingredient  \\\n","698  ['ground beef', 'ground pork', 'onion', 'garli...   \n","\n","                                      Ingredient_units  \\\n","698  ground beef, ground pork, onion, garlic cloves...   \n","\n","                                             Direction  \n","698  ['Combine first 8 ingredients in large saucepa...  \n","\n","Cosine Similarities (Numeric Features):\n","[[0.974178   0.95229378 0.95181954 0.94262861 0.93635933]]\n","\n","Cosine Similarities (Text Features):\n","[0.13370248]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the data\n","data = pd.read_csv('/content/drive/MyDrive/DS108/02.preprocess/dataset_final/dataset.csv')\n","\n","# Drop duplicate rows\n","data = data.drop_duplicates()\n","\n","# Insert 'Recipes ID' column\n","data.insert(0, 'Recipes ID', range(1, len(data) + 1))\n","\n","# Select relevant columns\n","columns = ['Recipes ID', 'Time (mins)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Cholesterol (mg)',\n","           'Sodium (mg)', 'Total Carbohydrate (g)', 'Dietary Fiber (g)', 'Sugars (g)', 'Protein (g)',\n","           'Diet Label', 'Ingredient', 'Ingredient_units', 'Direction']\n","dataset = data[columns]\n","\n","# Define maximum values for filtering\n","max_Calories = 2000  # kcal\n","max_daily_fat = 78  # grams\n","max_daily_Saturatedfat = 20  # grams\n","max_daily_Cholesterol = 300  # mg\n","max_daily_Sodium = 2300  # mg\n","max_daily_Carbohydrate = 275  # grams\n","max_daily_Fiber = 28  # grams\n","max_daily_Sugar = 50  # grams (added sugars)\n","max_daily_Protein = 50  # grams\n","\n","max_list = [max_Calories, max_daily_fat, max_daily_Saturatedfat, max_daily_Cholesterol, max_daily_Sodium, max_daily_Carbohydrate, max_daily_Fiber, max_daily_Sugar, max_daily_Protein]\n","\n","# Define functions\n","def scaling(dataframe):\n","    scaler = StandardScaler()\n","    prep_data = scaler.fit_transform(dataframe.iloc[:, 3:11])\n","    return prep_data, scaler\n","\n","def tfidf_transform(ingredient_units):\n","    tfidf = TfidfVectorizer()\n","    ingredient_units_tfidf = tfidf.fit_transform(ingredient_units)\n","    return ingredient_units_tfidf, tfidf\n","\n","def nn_predictor(prep_data):\n","    neigh = NearestNeighbors(metric='cosine', algorithm='brute')\n","    neigh.fit(prep_data)\n","    return neigh\n","\n","def build_pipeline(neigh, scaler, params):\n","    n_neighbors = min(params['n_neighbors'], len(scaler.transform(dataset.iloc[:, 3:11])))\n","    print(f\"Number of neighbors used: {n_neighbors}\")  # Add this line to check number of neighbors used\n","    transformer = FunctionTransformer(lambda X: neigh.kneighbors(X, n_neighbors=n_neighbors)[1], validate=False)\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('NN', transformer)\n","    ])\n","    return pipeline\n","\n","def extract_data(dataframe, ingredient_filter, max_nutritional_values):\n","    extracted_data = dataframe.copy()\n","    for column, maximum in zip(extracted_data.columns[3:11], max_nutritional_values):\n","        extracted_data = extracted_data[extracted_data[column] < maximum]\n","    print(f\"Filtered Data Shape: {extracted_data.shape}\")  # Add this line to check filtered data shape\n","    return extracted_data\n","\n","def apply_pipeline(pipeline, _input, extracted_data, scaler):\n","    indices = pipeline.transform(_input)[0]\n","    indices = indices.flatten()\n","    print(f\"Indices of Neighbors: {indices}\")  # Add this line to print the indices\n","    recommendations = extracted_data.iloc[indices]\n","    print(f\"Number of Recommendations: {len(recommendations)}\")  # Print number of recommendations\n","\n","    input_scaled = scaler.transform(_input)\n","    recommendations_scaled = scaler.transform(recommendations.iloc[:, 3:11])\n","    similarities = cosine_similarity(input_scaled, recommendations_scaled)\n","    print(f\"Similarities: {similarities}\")  # Print the similarities\n","\n","    return recommendations, similarities\n","\n","def recommand(dataframe, _input, max_nutritional_values, ingredient_filter=None, params={'n_neighbors': 5, 'return_distance': False}):\n","    extracted_data = extract_data(dataframe, ingredient_filter, max_nutritional_values)\n","    print(f\"Extracted Data Shape: {extracted_data.shape}\")  # Print shape of extracted data\n","    prep_data, scaler = scaling(extracted_data)\n","    ingredient_units_tfidf, tfidf = tfidf_transform(extracted_data['Ingredient_units'])\n","    neigh = nn_predictor(prep_data)\n","    pipeline = build_pipeline(neigh, scaler, params)\n","\n","    _input_scaled = scaler.transform(_input)\n","\n","    recommendations, similarities = apply_pipeline(pipeline, _input_scaled, extracted_data, scaler)\n","\n","    if ingredient_filter is not None:\n","        filter_vector = tfidf.transform([' '.join(ingredient_filter)])\n","        recommendations_tfidf = tfidf.transform(recommendations['Ingredient_units'])\n","        text_similarities = cosine_similarity(recommendations_tfidf, filter_vector).flatten()\n","\n","        if len(recommendations) == len(text_similarities):\n","            recommendations = recommendations[text_similarities > 0.0]\n","            text_similarities = text_similarities[text_similarities > 0.0]\n","        else:\n","            print(\"Warning: Length mismatch between recommendations and text similarities\")\n","            text_similarities = None\n","    else:\n","        text_similarities = None\n","\n","    print(\"Recommended Recipes:\")\n","    print(recommendations)\n","    print(\"\\nCosine Similarities (Numeric Features):\")\n","    print(similarities)\n","    if text_similarities is not None:\n","        print(\"\\nCosine Similarities (Text Features):\")\n","        print(text_similarities)\n","\n","    return recommendations, similarities, text_similarities\n","\n","# Run model\n","test_input = dataset.iloc[0:1, 3:11].to_numpy()\n","ingredient_filter = ['garlic']\n","recommendations, similarities, text_similarities = recommand(dataset, test_input, max_list, ingredient_filter=ingredient_filter)"]},{"cell_type":"code","source":["relevance_scores = np.zeros(len(recommendations))\n","relevance_scores[:2] = 1\n","\n","ndcg_score = calculate_ndcg(recommendations, relevance_scores, k=10)\n","print(f\"NDCG Score of KNN: {ndcg_score}\")\n","\n","coverage_score = calculate_coverage(recommendations, len(dataset))\n","print(f\"Coverage Score of KNN: {coverage_score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0L7nN3T14Ab8","executionInfo":{"status":"ok","timestamp":1719244484849,"user_tz":-420,"elapsed":356,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"7a7ef71b-805a-4eb7-9ae0-9cb5ca2e017f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NDCG Score of KNN: 1.0\n","Coverage Score of KNN: 9.738046547862498e-05\n"]}]},{"cell_type":"markdown","source":["#GMM"],"metadata":{"id":"wvj6PrpP1kjm"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.mixture import GaussianMixture\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the data\n","data = pd.read_csv('/content/drive/MyDrive/DS108/02.preprocess/dataset_final/dataset.csv')\n","\n","# Drop duplicate rows\n","data = data.drop_duplicates()\n","\n","# Insert 'Recipes ID' column\n","data.insert(0, 'Recipes ID', range(1, len(data) + 1))\n","\n","# Select relevant columns\n","columns = ['Recipes ID', 'Time (mins)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Cholesterol (mg)',\n","           'Sodium (mg)', 'Total Carbohydrate (g)', 'Dietary Fiber (g)', 'Sugars (g)', 'Protein (g)',\n","           'Diet Label', 'Ingredient', 'Ingredient_units', 'Direction']\n","dataset = data[columns]\n","\n","# Define maximum values for filtering\n","max_Calories = 2000  # kcal\n","max_daily_fat = 78  # grams\n","max_daily_Saturatedfat = 20  # grams\n","max_daily_Cholesterol = 300  # mg\n","max_daily_Sodium = 2300  # mg\n","max_daily_Carbohydrate = 275  # grams\n","max_daily_Fiber = 28  # grams\n","max_daily_Sugar = 50  # grams (added sugars)\n","max_daily_Protein = 50  # grams\n","\n","max_list = [max_Calories, max_daily_fat, max_daily_Saturatedfat, max_daily_Cholesterol, max_daily_Sodium, max_daily_Carbohydrate, max_daily_Fiber, max_daily_Sugar, max_daily_Protein]\n","\n","# Define functions\n","def scaling(dataframe):\n","    scaler = StandardScaler()\n","    prep_data = scaler.fit_transform(dataframe.iloc[:, 3:11])\n","    return prep_data, scaler\n","\n","def tfidf_transform(ingredient_units):\n","    tfidf = TfidfVectorizer()\n","    ingredient_units_tfidf = tfidf.fit_transform(ingredient_units)\n","    return ingredient_units_tfidf, tfidf\n","\n","def gmm_predictor(prep_data, n_components):\n","    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0)\n","    gmm.fit(prep_data)\n","    return gmm\n","\n","def build_pipeline(gmm, scaler, params):\n","    transformer = FunctionTransformer(lambda X: gmm.predict(X), validate=False)\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('GMM', transformer)\n","    ])\n","    return pipeline\n","\n","def extract_data(dataframe, ingredient_filter, max_nutritional_values):\n","    extracted_data = dataframe.copy()\n","    for column, maximum in zip(extracted_data.columns[3:11], max_nutritional_values):\n","        extracted_data = extracted_data[extracted_data[column] < maximum]\n","    return extracted_data\n","\n","def apply_pipeline(pipeline, _input, extracted_data, scaler):\n","    cluster_labels = pipeline.transform(_input)\n","    cluster_indices = np.where(cluster_labels == cluster_labels[0])[0]\n","    recommendations = extracted_data.iloc[cluster_indices]\n","\n","    input_scaled = scaler.transform(_input)\n","    recommendations_scaled = scaler.transform(recommendations.iloc[:, 3:11])\n","    similarities = cosine_similarity(input_scaled, recommendations_scaled)\n","\n","    return recommendations, similarities\n","\n","def recommand(dataframe, _input, max_nutritional_values, ingredient_filter=None, params={'n_components': 5}):\n","    extracted_data = extract_data(dataframe, ingredient_filter, max_nutritional_values)\n","    prep_data, scaler = scaling(extracted_data)\n","    ingredient_units_tfidf, tfidf = tfidf_transform(extracted_data['Ingredient_units'])\n","    gmm = gmm_predictor(prep_data, params['n_components'])\n","    pipeline = build_pipeline(gmm, scaler, params)\n","\n","    _input_scaled = scaler.transform(_input)\n","\n","    recommendations, similarities = apply_pipeline(pipeline, _input_scaled, extracted_data, scaler)\n","\n","    if ingredient_filter is not None:\n","        filter_vector = tfidf.transform([' '.join(ingredient_filter)])\n","        recommendations_tfidf = tfidf.transform(recommendations['Ingredient_units'])\n","        text_similarities = cosine_similarity(recommendations_tfidf, filter_vector).flatten()\n","\n","        # Ensure the length matches before filtering\n","        if len(recommendations) == len(text_similarities):\n","            recommendations = recommendations[text_similarities > 0.0]\n","            text_similarities = text_similarities[text_similarities > 0.0]\n","        else:\n","            print(\"Warning: Length mismatch between recommendations and text similarities\")\n","            text_similarities = None\n","    else:\n","        text_similarities = None\n","\n","    print(\"Recommended Recipes:\")\n","    print(recommendations)\n","    print(\"\\nCosine Similarities (Numeric Features):\")\n","    print(similarities)\n","    if text_similarities is not None:\n","        print(\"\\nCosine Similarities (Text Features):\")\n","        print(text_similarities)\n","\n","    return recommendations, similarities, text_similarities\n","\n","# Run model\n","test_input = dataset.iloc[0:1, 3:11].to_numpy()\n","ingredient_filter = ['garlic']\n","recommand(dataset, test_input, max_list, ingredient_filter=ingredient_filter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXzx0s0RyKaw","executionInfo":{"status":"ok","timestamp":1718117515341,"user_tz":-420,"elapsed":1775,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"808c2463-b5d4-49ea-92f4-8e4269af1d00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Recommended Recipes:\n","Empty DataFrame\n","Columns: [Recipes ID, Time (mins), Calories, Total Fat (g), Saturated Fat (g), Cholesterol (mg), Sodium (mg), Total Carbohydrate (g), Dietary Fiber (g), Sugars (g), Protein (g), Diet Label, Ingredient, Ingredient_units, Direction]\n","Index: []\n","\n","Cosine Similarities (Numeric Features):\n","[[0.05496253]]\n","\n","Cosine Similarities (Text Features):\n","[]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["(Empty DataFrame\n"," Columns: [Recipes ID, Time (mins), Calories, Total Fat (g), Saturated Fat (g), Cholesterol (mg), Sodium (mg), Total Carbohydrate (g), Dietary Fiber (g), Sugars (g), Protein (g), Diet Label, Ingredient, Ingredient_units, Direction]\n"," Index: [],\n"," array([[0.05496253]]),\n"," array([], dtype=float64))"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["#Radius Neighbors Classifier"],"metadata":{"id":"OEZT3Qt1ALfF"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.neighbors import RadiusNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the data\n","data = pd.read_csv('/content/drive/MyDrive/DS108/02.preprocess/dataset_final/dataset.csv')\n","\n","# Drop duplicate rows\n","data = data.drop_duplicates()\n","\n","# Insert 'Recipes ID' column\n","data.insert(0, 'Recipes ID', range(1, len(data) + 1))\n","\n","# Select relevant columns\n","columns = ['Recipes ID', 'Time (mins)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Cholesterol (mg)',\n","           'Sodium (mg)', 'Total Carbohydrate (g)', 'Dietary Fiber (g)', 'Sugars (g)', 'Protein (g)',\n","           'Diet Label', 'Ingredient', 'Ingredient_units', 'Direction']\n","dataset = data[columns]\n","\n","# Define maximum values for filtering\n","max_Calories = 2000  # kcal\n","max_daily_fat = 78  # grams\n","max_daily_Saturatedfat = 20  # grams\n","max_daily_Cholesterol = 300  # mg\n","max_daily_Sodium = 2300  # mg\n","max_daily_Carbohydrate = 275  # grams\n","max_daily_Fiber = 28  # grams\n","max_daily_Sugar = 50  # grams (added sugars)\n","max_daily_Protein = 50  # grams\n","\n","max_list = [max_Calories, max_daily_fat, max_daily_Saturatedfat, max_daily_Cholesterol, max_daily_Sodium, max_daily_Carbohydrate, max_daily_Fiber, max_daily_Sugar, max_daily_Protein]\n","\n","# Define functions\n","def scaling(dataframe):\n","    scaler = StandardScaler()\n","    prep_data = scaler.fit_transform(dataframe.iloc[:, 3:11])\n","    return prep_data, scaler\n","\n","def radius_neighbors_fit(prep_data, params):\n","    radius_neighbors = RadiusNeighborsClassifier(radius=params['radius'], algorithm='brute', metric='cosine')\n","    radius_neighbors.fit(prep_data, np.zeros(len(prep_data)))  # RadiusNeighborsClassifier yêu cầu một mảng nhãn, ta sử dụng mảng gồm toàn số 0\n","    return radius_neighbors\n","\n","def extract_data(dataframe, max_nutritional_values):\n","    extracted_data = dataframe.copy()\n","    for column, maximum in zip(extracted_data.columns[3:11], max_nutritional_values):\n","        extracted_data = extracted_data[extracted_data[column] < maximum]\n","    return extracted_data\n","\n","def apply_radius_neighbors(radius_neighbors, _input, extracted_data, scaler, params):\n","    input_scaled = scaler.transform(_input)\n","    indices = radius_neighbors.radius_neighbors(input_scaled, return_distance=False)\n","    indices = indices[0]  # Vì chỉ cần một kết quả, ta lấy phần tử đầu tiên\n","    recommendations = extracted_data.iloc[indices]\n","\n","    recommendations_scaled = scaler.transform(recommendations.iloc[:, 3:11])\n","    similarities = cosine_similarity(input_scaled, recommendations_scaled)\n","\n","    return recommendations, similarities\n","\n","def compute_text_similarity(dataframe, input_text, tfidf):\n","    recommendations_tfidf = tfidf.transform(dataframe['Ingredient_units'])\n","    text_similarities = cosine_similarity(recommendations_tfidf, input_text).flatten()\n","    return text_similarities\n","\n","def recommend(dataframe, _input, max_nutritional_values, ingredient_filter=None, params={'radius': 0.5}):\n","    extracted_data = extract_data(dataframe, max_nutritional_values)\n","    prep_data, scaler = scaling(extracted_data)\n","    radius_neighbors = radius_neighbors_fit(prep_data, params)\n","\n","    recommendations, numeric_similarities = apply_radius_neighbors(radius_neighbors, _input, extracted_data, scaler, params)\n","\n","    if ingredient_filter is not None:\n","        tfidf = TfidfVectorizer()\n","        tfidf.fit(extracted_data['Ingredient_units'])\n","\n","        transformed_input = tfidf.transform([' '.join(ingredient_filter)])\n","\n","        text_similarities = compute_text_similarity(recommendations, transformed_input, tfidf)\n","\n","        recommendations['Numeric Similarity'] = numeric_similarities.flatten()\n","        recommendations['Text Similarity'] = text_similarities\n","\n","        recommendations_sorted = recommendations.sort_values(by='Numeric Similarity', ascending=False)\n","\n","        top_recommendations = recommendations_sorted.head(5)\n","\n","        print(\"Top Recommended Recipes:\")\n","        print(top_recommendations[['Recipes ID', 'Numeric Similarity', 'Text Similarity']])\n","    else:\n","        print(\"Ingredient filter is not provided. Please provide an ingredient filter.\")\n","        return None\n","\n","    relevance_scores = np.zeros(len(top_recommendations))\n","    relevance_scores[:2] = 1\n","\n","    # Calculate NDCG\n","    ndcg_score = calculate_ndcg(top_recommendations, relevance_scores, k=5)\n","    # print(f\"NDCG Score: {ndcg_score}\")\n","\n","    # Calculate Coverage\n","    coverage_score = calculate_coverage(recommendations, len(dataset))\n","    # print(f\"Coverage Score: {coverage_score}\")\n","\n","    return top_recommendations, ndcg_score, coverage_score\n","\n","# Run model\n","test_input = dataset.iloc[0:1, 3:11].to_numpy()\n","ingredient_filter = ['garlic']\n","recommendations, ndcg_score, coverage_score = recommend(dataset, test_input, max_list, ingredient_filter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnvJM4XDK4VH","executionInfo":{"status":"ok","timestamp":1719244886087,"user_tz":-420,"elapsed":1558,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"ba2e901d-6319-4d68-ad07-96a7304889e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top Recommended Recipes:\n","      Recipes ID  Numeric Similarity  Text Similarity\n","6073        6074            0.924459         0.188852\n","6732        6733            0.906683         0.000000\n","3479        3480            0.894694         0.151270\n","320          321            0.862496         0.166101\n","2030        2031            0.861416         0.000000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","<ipython-input-10-a8717067b589>:84: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  recommendations['Numeric Similarity'] = numeric_similarities.flatten()\n","<ipython-input-10-a8717067b589>:85: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  recommendations['Text Similarity'] = text_similarities\n"]}]},{"cell_type":"code","source":["print(\"NDCG Score RNC:\", ndcg_score)\n","print(\"Coverage Score RNC:\", coverage_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8R3bG-H9ctY","executionInfo":{"status":"ok","timestamp":1719244893490,"user_tz":-420,"elapsed":398,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"357dd34f-b739-470e-c042-536b3b1cac1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NDCG Score RNC: 0.5531464700081437\n","Coverage Score RNC: 0.03807576200214237\n"]}]},{"cell_type":"markdown","source":["#K Means"],"metadata":{"id":"uM0ByJBY_GEw"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the data\n","data = pd.read_csv('/content/drive/MyDrive/DS108/02.preprocess/dataset_final/dataset.csv')\n","\n","# Drop duplicate rows\n","data = data.drop_duplicates()\n","\n","# Insert 'Recipes ID' column\n","data.insert(0, 'Recipes ID', range(1, len(data) + 1))\n","\n","# Select relevant columns\n","columns = ['Recipes ID', 'Time (mins)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Cholesterol (mg)',\n","           'Sodium (mg)', 'Total Carbohydrate (g)', 'Dietary Fiber (g)', 'Sugars (g)', 'Protein (g)',\n","           'Diet Label', 'Ingredient', 'Ingredient_units', 'Direction']\n","dataset = data[columns]\n","\n","# Define maximum values for filtering\n","max_Calories = 2000  # kcal\n","max_daily_fat = 78  # grams\n","max_daily_Saturatedfat = 20  # grams\n","max_daily_Cholesterol = 300  # mg\n","max_daily_Sodium = 2300  # mg\n","max_daily_Carbohydrate = 275  # grams\n","max_daily_Fiber = 28  # grams\n","max_daily_Sugar = 50  # grams (added sugars)\n","max_daily_Protein = 50  # grams\n","\n","max_list = [max_Calories, max_daily_fat, max_daily_Saturatedfat, max_daily_Cholesterol, max_daily_Sodium, max_daily_Carbohydrate, max_daily_Fiber, max_daily_Sugar, max_daily_Protein]\n","\n","# Define functions\n","def scaling(dataframe):\n","    scaler = StandardScaler()\n","    prep_data = scaler.fit_transform(dataframe.iloc[:, 3:11])\n","    return prep_data, scaler\n","\n","def kmeans_fit(prep_data, params):\n","    kmeans = KMeans(n_clusters=params['n_clusters'])\n","    kmeans.fit(prep_data)\n","    return kmeans\n","\n","def build_pipeline(kmeans, scaler):\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('kmeans', kmeans)\n","    ])\n","    return pipeline\n","\n","def extract_data(dataframe, max_nutritional_values):\n","    extracted_data = dataframe.copy()\n","    for column, maximum in zip(extracted_data.columns[3:11], max_nutritional_values):\n","        extracted_data = extracted_data[extracted_data[column] < maximum]\n","    return extracted_data\n","\n","def apply_pipeline(pipeline, _input, extracted_data, scaler):\n","    input_scaled = scaler.transform(_input)\n","    cluster_labels = pipeline.predict(input_scaled)\n","    indices = np.where(pipeline.named_steps['kmeans'].labels_ == cluster_labels[0])[0]\n","    recommendations = extracted_data.iloc[indices]\n","\n","    input_scaled = scaler.transform(_input)\n","    recommendations_scaled = scaler.transform(recommendations.iloc[:, 3:11])\n","\n","    return recommendations\n","\n","def recommend(dataframe, _input, max_nutritional_values, params={'n_clusters': 5}):\n","    extracted_data = extract_data(dataframe, max_nutritional_values)\n","    prep_data, scaler = scaling(extracted_data)\n","    kmeans = kmeans_fit(prep_data, params)\n","    pipeline = build_pipeline(kmeans, scaler)\n","\n","    recommendations = apply_pipeline(pipeline, _input, extracted_data, scaler)\n","    # Lấy chỉ 5 công thức hàng đầu\n","    top_recommendations = recommendations.head(5)\n","\n","    print(\"Top Recommended Recipes:\")\n","    print(top_recommendations)\n","\n","    relevance_scores = np.zeros(len(top_recommendations))\n","\n","    # Calculate NDCG\n","    ndcg_score = calculate_ndcg(top_recommendations, relevance_scores, k=5)\n","    # print(f\"NDCG Score: {ndcg_score}\")\n","\n","    # Calculate Coverage\n","    coverage_score = calculate_coverage(recommendations, len(dataset))\n","    # print(f\"Coverage Score: {coverage_score}\")\n","\n","    return top_recommendations, ndcg_score, coverage_score\n","\n","# Run model\n","test_input = dataset.iloc[0:1, 3:11].to_numpy()\n","recommendations, ndcg_score, coverage_score = recommend(dataset, test_input, max_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSq-4u1R_CcD","executionInfo":{"status":"ok","timestamp":1719245071295,"user_tz":-420,"elapsed":841,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"34f1f8e6-b618-47a4-de31-0b8397c95994"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Top Recommended Recipes:\n","    Recipes ID  Time (mins)  Calories  Total Fat (g)  Saturated Fat (g)  \\\n","26          27         25.0      17.7            1.7                1.1   \n","50          51         30.0      45.2            1.6                0.5   \n","56          57         45.0     114.3            8.6                2.0   \n","59          60         22.0      65.3            3.8                0.5   \n","75          76         32.0      22.0            0.1                0.0   \n","\n","    Cholesterol (mg)  Sodium (mg)  Total Carbohydrate (g)  Dietary Fiber (g)  \\\n","26               4.4        101.1                     0.7                0.2   \n","50               1.8        145.9                     6.5                2.2   \n","56               4.9        115.1                     6.5                2.0   \n","59               0.0         22.8                     7.4                1.4   \n","75               0.0         34.6                     5.0                1.5   \n","\n","    Sugars (g)  Protein (g)      Diet Label  \\\n","26         0.1          0.1     gluten_free   \n","50         1.8          2.4        low_carb   \n","56         2.0          4.3        low_carb   \n","59         0.8          2.0        low_carb   \n","75         2.5          1.0  weigh_watchers   \n","\n","                                           Ingredient  \\\n","26  ['2 tablespoons butter', '1 garlic clove, minc...   \n","50  ['2 teaspoons sesame seeds', '2 tablespoons ve...   \n","56  ['2 cups cauliflower, finely chopped fresh or ...   \n","59  ['1 tablespoon minced fresh gingerroot', '1⁄2 ...   \n","75  ['1⁄4 cup parmesan cheese, grated', '2 tablesp...   \n","\n","                                     Ingredient_units  \\\n","26  ['butter', 'garlic', 'onion powder', 'lemon ju...   \n","50  ['sesame seeds', 'vegetable oil', 'garlic', 'f...   \n","56       ['cauliflower', 'real sour cream', 'butter']   \n","59  ['fresh gingerroot', 'safflower oil', 'kale', ...   \n","75  ['parmesan cheese', 'dried Italian seasoned br...   \n","\n","                                            Direction  \n","26  ['Combine ingredients and simmer covered for a...  \n","50  [\"In a thick bottom frying pan, grill the sasa...  \n","56  ['Steam or microwave cauliflower until Very so...  \n","59  ['In a medium skillet, cook the ginger root in...  \n","75  ['Preheat oven to 400 degrees.', 'In resealabl...  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["print(\"NDCG Score K Means:\", ndcg_score)\n","print(\"Coverage Score K Means:\", coverage_score)"],"metadata":{"id":"QYckrh2l_epr","executionInfo":{"status":"ok","timestamp":1719245075739,"user_tz":-420,"elapsed":323,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"d633a88c-5397-4c20-c700-cc0b3c3d6d24","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NDCG Score K Means: 0.0\n","Coverage Score K Means: 0.09845165059888986\n"]}]},{"cell_type":"markdown","source":["#DBSCAN"],"metadata":{"id":"RZy1A7eI1fCO"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.cluster import DBSCAN\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.preprocessing import FunctionTransformer, StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the data\n","data = pd.read_csv('/content/drive/MyDrive/DS108/02.preprocess/dataset_final/dataset.csv')\n","\n","# Drop duplicate rows\n","data = data.drop_duplicates()\n","\n","# Insert 'Recipes ID' column\n","data.insert(0, 'Recipes ID', range(1, len(data) + 1))\n","\n","# Select relevant columns\n","columns = ['Recipes ID', 'Time (mins)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Cholesterol (mg)',\n","           'Sodium (mg)', 'Total Carbohydrate (g)', 'Dietary Fiber (g)', 'Sugars (g)', 'Protein (g)',\n","           'Diet Label', 'Ingredient', 'Ingredient_units', 'Direction']\n","dataset = data[columns]\n","\n","# Define maximum values for filtering\n","max_Calories = 2000  # kcal\n","max_daily_fat = 78  # grams\n","max_daily_Saturatedfat = 20  # grams\n","max_daily_Cholesterol = 300  # mg\n","max_daily_Sodium = 2300  # mg\n","max_daily_Carbohydrate = 275  # grams\n","max_daily_Fiber = 28  # grams\n","max_daily_Sugar = 50  # grams (added sugars)\n","max_daily_Protein = 50  # grams\n","\n","max_list = [max_Calories, max_daily_fat, max_daily_Saturatedfat, max_daily_Cholesterol, max_daily_Sodium, max_daily_Carbohydrate, max_daily_Fiber, max_daily_Sugar, max_daily_Protein]\n","\n","# Define functions\n","def scaling(dataframe):\n","    if len(dataframe) == 0:\n","        return None, None\n","    scaler = StandardScaler()\n","    prep_data = scaler.fit_transform(dataframe.iloc[:, 3:11])\n","    return prep_data, scaler\n","\n","def tfidf_transform(ingredient_units):\n","    tfidf = TfidfVectorizer()\n","    ingredient_units_tfidf = tfidf.fit_transform(ingredient_units)\n","    return ingredient_units_tfidf, tfidf\n","\n","def dbscan_predictor(prep_data, eps, min_samples):\n","    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n","    dbscan.fit(prep_data)\n","    return dbscan\n","\n","def build_pipeline(dbscan, scaler, params):\n","    cluster_labels = dbscan.labels_\n","    transformer = FunctionTransformer(lambda X: [cluster_labels], validate=False)\n","    pipeline = Pipeline([\n","        ('scaler', scaler),\n","        ('DBSCAN', transformer)\n","    ])\n","    return pipeline\n","\n","def extract_data(dataframe, ingredient_filter, max_nutritional_values):\n","    extracted_data = dataframe.copy()\n","    for column, maximum in zip(extracted_data.columns[3:11], max_nutritional_values):\n","        extracted_data = extracted_data[extracted_data[column] < maximum]\n","    return extracted_data\n","\n","def apply_pipeline(pipeline, _input, extracted_data, scaler, dbscan):\n","    cluster_labels = dbscan.labels_\n","    input_scaled = scaler.transform(_input)\n","    input_label = dbscan.fit_predict(input_scaled)\n","\n","    # Find the cluster of the input data point\n","    cluster_indices = np.where(cluster_labels == input_label[0])[0]\n","    if len(cluster_indices) == 0:\n","        return None, None\n","\n","    recommendations = extracted_data.iloc[cluster_indices]\n","    recommendations_scaled = scaler.transform(recommendations.iloc[:, 3:11])\n","    similarities = cosine_similarity(input_scaled, recommendations_scaled)\n","\n","    return recommendations, similarities\n","\n","def recommand(dataframe, _input, max_nutritional_values, ingredient_filter=None, params={'eps': 0.5, 'min_samples': 5}):\n","    extracted_data = extract_data(dataframe, ingredient_filter, max_nutritional_values)\n","    if len(extracted_data) == 0:\n","        print(\"No data available after filtering\")\n","        return None, None, None\n","\n","    prep_data, scaler = scaling(extracted_data)\n","    if prep_data is None or scaler is None:\n","        print(\"No data available for scaling\")\n","        return None, None, None\n","\n","    ingredient_units_tfidf, tfidf = tfidf_transform(extracted_data['Ingredient_units'])\n","    dbscan = dbscan_predictor(prep_data, params['eps'], params['min_samples'])\n","    pipeline = build_pipeline(dbscan, scaler, params)\n","\n","    _input_scaled = scaler.transform(_input)\n","\n","    recommendations, similarities = apply_pipeline(pipeline, _input_scaled, extracted_data, scaler, dbscan)\n","\n","    if recommendations is None or similarities is None:\n","        print(\"No recommendations found\")\n","        return None, None, None\n","\n","    if ingredient_filter is not None:\n","        filter_vector = tfidf.transform([' '.join(ingredient_filter)])\n","        recommendations_tfidf = tfidf.transform(recommendations['Ingredient_units'])\n","        if recommendations_tfidf.shape[0] == 0:\n","            print(\"No recommendations found after filtering by ingredients\")\n","            return recommendations, similarities, None\n","\n","        text_similarities = cosine_similarity(recommendations_tfidf, filter_vector).flatten()\n","\n","        # Ensure the length matches before filtering\n","        if len(recommendations) == len(text_similarities):\n","            recommendations = recommendations[text_similarities > 0.0]\n","            text_similarities = text_similarities[text_similarities > 0.0]\n","        else:\n","            print(\"Warning: Length mismatch between recommendations and text similarities\")\n","            text_similarities = None\n","    else:\n","        text_similarities = None\n","\n","    print(\"Recommended Recipes:\")\n","    print(recommendations)\n","    print(\"\\nCosine Similarities (Numeric Features):\")\n","    print(similarities)\n","    if text_similarities is not None:\n","        print(\"\\nCosine Similarities (Text Features):\")\n","        print(text_similarities)\n","\n","    return recommendations, similarities, text_similarities\n","\n","# Run model\n","test_input = dataset.iloc[0:1, 3:11].to_numpy()\n","ingredient_filter = ['garlic']\n","recommand(dataset, test_input, max_list, ingredient_filter=ingredient_filter, params={'eps': 0.5, 'min_samples': 5})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvlhV3vsyn7R","executionInfo":{"status":"ok","timestamp":1718118348388,"user_tz":-420,"elapsed":1887,"user":{"displayName":"Meo Nhóm","userId":"17799391767858028047"}},"outputId":"88c8ce8a-cfe3-4367-ffe3-0ce206e52721"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No recommendations found\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["(None, None, None)"]},"metadata":{},"execution_count":11}]}]}